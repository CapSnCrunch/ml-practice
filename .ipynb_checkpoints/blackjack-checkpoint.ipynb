{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53d0b239-b886-4dcb-8d75-ba2b4269d32a",
   "metadata": {},
   "source": [
    "# Blackjack Implementation\n",
    "\n",
    "Assumptions:\n",
    "- Drawing from an infinite deck (doesn't matter if we count cards)\n",
    "- Useable Ace vs Ace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a549786e-c458-411d-8693-5be7825aef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Player():\n",
    "    def __init__(self):\n",
    "        self.cards = np.array([1, 2], dtype=\"int\")\n",
    "        self.hit(2)\n",
    "    \n",
    "    def value(self):\n",
    "        hand_sum = np.sum(self.cards)\n",
    "        if 1 not in self.cards:\n",
    "            return (hand_sum, False)\n",
    "\n",
    "        aces = np.bincount(self.cards)[1]\n",
    "        usable_ace = hand_sum - (aces - 1) <= 10\n",
    "        if usable_ace:\n",
    "            return (hand_sum + 10, True)\n",
    "        \n",
    "        return (hand_sum, False)\n",
    "        \n",
    "    def hit(self, n=1):\n",
    "        for i in range(n):\n",
    "            random_card = np.random.choice([x for x in range(1, 11)], 1, p=[1/13 for x in range(1, 10)] + [4/13])\n",
    "            self.cards = np.append(self.cards, [random_card])\n",
    "\n",
    "    def bust(self):\n",
    "        return self.value()[0] > 21\n",
    "\n",
    "class Blackjack():\n",
    "    def __init__(self):\n",
    "        self.player = Player()\n",
    "        self.dealer = Player()\n",
    "\n",
    "    def reward(self):\n",
    "        if self.player.bust():\n",
    "            return -1\n",
    "\n",
    "        if self.dealer.bust():\n",
    "            return 1\n",
    "\n",
    "        if self.player.value()[0] >= self.dealer.value()[0]:\n",
    "            return 1\n",
    "\n",
    "        return 0\n",
    "    \n",
    "    def play(self, policy):\n",
    "\n",
    "        episode = []\n",
    "        # Player's Turn\n",
    "        state = None\n",
    "        player_hit = False\n",
    "        while True:\n",
    "            p_val, useable_ace = self.player.value()\n",
    "            state = (p_val, useable_ace, self.dealer.cards[0])\n",
    "            player_hit = policy.action(state)\n",
    "            if player_hit:\n",
    "                self.player.hit()\n",
    "                if self.player.bust():\n",
    "                    episode.append((state, player_hit, -1))\n",
    "                    return episode\n",
    "                episode.append((state, player_hit, 0))\n",
    "                continue\n",
    "            break\n",
    "            \n",
    "        # Dealer's Turn\n",
    "        while True:\n",
    "            dealer_hit = self.dealer.value()[0] < 17\n",
    "            if dealer_hit:\n",
    "                self.dealer.hit()\n",
    "                if self.dealer.bust():\n",
    "                    break\n",
    "                continue\n",
    "            break\n",
    "\n",
    "        episode.append((state, player_hit, self.reward()))\n",
    "        return episode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdc34d5-3487-4a33-ad0a-7e93e6f0221c",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "54140079-b773-45b1-823e-315186b7c184",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy():\n",
    "    def action_prob(self,state:int,action:int) -> float:\n",
    "        \"\"\"\n",
    "        input:\n",
    "            state, action\n",
    "        return:\n",
    "            \\pi(a|s)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def action(self,state:int) -> int:\n",
    "        \"\"\"\n",
    "        input:\n",
    "            state\n",
    "        return:\n",
    "            action\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3da01d-ef99-46a5-a73b-3280cd19b880",
   "metadata": {},
   "source": [
    "# Monte Carlo\n",
    "\n",
    "## Variations\n",
    "- Exploring Starts (for pi)\n",
    "    - First-Visit MC (for v)\n",
    "    - Every-Visit MC (for v)\n",
    "- On-Policy MC Control (for pi)\n",
    "    - First-Visit MC (for v)\n",
    "    - Every-Visit MC (for v)\n",
    "- Off-Policy (for pi) (Importance Sampling)\n",
    "    - MC Control\n",
    "    - Incremental Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e9ac6c8b-3436-47d1-844c-14a3cb45ab7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "class behavior_policy(Policy):\n",
    "    def __init__(self):\n",
    "        self.p = 0.5\n",
    "\n",
    "    def action_prob(self,state:Tuple[int,int,int],action:int):\n",
    "        return self.p\n",
    "\n",
    "    def action(self, state:Tuple[int,int,int]):\n",
    "        return np.random.uniform(0, 1) > self.p\n",
    "\n",
    "class target_policy(Policy):\n",
    "    def __init__(self, Q):\n",
    "        self.p = np.zeros(Q.shape)\n",
    "        aArgmax = np.argmax(Q, axis=3)\n",
    "        i, j, k = np.indices(aArgmax.shape)\n",
    "        self.p[i, j, k, aArgmax] = 1\n",
    "        \n",
    "    def action_prob(self,state:Tuple[int,int,int],action:int):\n",
    "        player_value, usable_ace, dealer_showing = state\n",
    "        return self.p[player_value, usable_ace, dealer_showing, action]\n",
    "        \n",
    "    def action(self,state:Tuple[int,int,int]):\n",
    "        i, j, k = state\n",
    "        j = int(j)\n",
    "        return np.argmax(self.p[i, j, k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ce001410-c57c-4758-8732-359aa9fc1e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = []\n",
    "b = behavior_policy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8a23c3e7-15ea-4297-ac00-46bb60c79df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarlo():\n",
    "    def __init__(self):\n",
    "        self.Q = np.zeros((31, 2, 11, 2))\n",
    "        self.C = np.zeros((31, 2, 11, 2))\n",
    "        self.pi = target_policy(self.Q)\n",
    "        self.b = behavior_policy()\n",
    "        self.gamma = 1\n",
    "\n",
    "    def estimate(self, n_episodes=1000):\n",
    "        for i in range(n_episodes):\n",
    "            if(i % 1000 == 0):\n",
    "                print(i)\n",
    "            game = Blackjack()\n",
    "            episode = game.play(b)\n",
    "            \n",
    "            G = 0\n",
    "            W = 1\n",
    "            for (index, turn) in enumerate(episode):\n",
    "                S, A, R = turn\n",
    "                player_v, usable_ace, dealer_showing = S\n",
    "                A = int(A)\n",
    "                usable_ace = int(usable_ace)\n",
    "                G = self.gamma * G + R\n",
    "                C = self.C[player_v, usable_ace, dealer_showing, int(A)]\n",
    "                self.C[player_v, usable_ace, dealer_showing, int(A)] = C + W\n",
    "\n",
    "                Q = self.Q[player_v, usable_ace, dealer_showing, int(A)]\n",
    "                C = self.C[player_v, usable_ace, dealer_showing, int(A)]\n",
    "                self.Q[player_v, usable_ace, dealer_showing, int(A)] = Q + (W / C) * (G - Q)\n",
    "\n",
    "                aArgmax = np.argmax(self.Q[player_v, usable_ace, dealer_showing])\n",
    "                self.pi.p[player_v, usable_ace, dealer_showing] = np.zeros(self.pi.p[player_v, usable_ace, dealer_showing].shape) # Reset\n",
    "                self.pi.p[player_v, usable_ace, dealer_showing, aArgmax] = 1\n",
    "                \n",
    "                if int(A) != aArgmax:\n",
    "                    continue\n",
    "\n",
    "                W = W / self.b.action_prob(S, int(A))\n",
    "\n",
    "        return self.pi\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce0a4c2-7fe2-43b6-a4a4-15159c1ae525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.609\n"
     ]
    }
   ],
   "source": [
    "learner = MonteCarlo()\n",
    "\n",
    "wins = 0\n",
    "n_episodes = 1000\n",
    "for i in range(n_episodes):\n",
    "    game = Blackjack()\n",
    "    episode = game.play(learner.pi)\n",
    "    if episode[-1][2]:\n",
    "        wins = wins + 1\n",
    "        \n",
    "print(wins / n_episodes)\n",
    "\n",
    "player_policy = learner.estimate(100000)\n",
    "wins = 0\n",
    "n_episodes = 1000\n",
    "for i in range(n_episodes):\n",
    "    game = Blackjack()\n",
    "    episode = game.play(player_policy)\n",
    "    if episode[-1][2]:\n",
    "        wins = wins + 1\n",
    "\n",
    "print(wins / n_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759cc920-2d78-48e6-906f-cbe0d01fdf38",
   "metadata": {},
   "source": [
    "# Temporal Difference\n",
    "\n",
    "## Sarsa\n",
    "\n",
    "## Q-Learning\n",
    "\n",
    "## Expected Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65a661d-fc35-48b6-859c-1835f446b0b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
